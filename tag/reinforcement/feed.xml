<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://ilvnax24er.github.io/tag/reinforcement/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://ilvnax24er.github.io/" rel="alternate" type="text/html" />
  <updated>2021-07-06T19:32:57+09:00</updated>
  <id>https://ilvnax24er.github.io/tag/reinforcement/feed.xml</id>

  
  
  

  
    <title type="html">Archive | </title>
  

  
    <subtitle>Deeplearning Archive</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">Reinforcement Learning - Slot machine</title>
      <link href="https://ilvnax24er.github.io/2slotmachine" rel="alternate" type="text/html" title="Reinforcement Learning - Slot machine" />
      <published>2021-07-05T00:00:00+09:00</published>
      <updated>2021-07-05T00:00:00+09:00</updated>
      <id>https://ilvnax24er.github.io/2slotmachine</id>
      <content type="html" xml:base="https://ilvnax24er.github.io/2slotmachine">&lt;h2 id=&quot;첫-ai-모델-만들기---slot-machine&quot;&gt;첫 AI 모델 만들기 - Slot machine&lt;/h2&gt;

&lt;h3 id=&quot;다중-슬롯머신-문제&quot;&gt;다중 슬롯머신 문제&lt;/h3&gt;

&lt;p&gt;5개의 슬롯머신이 있는 방에 있다. 각 슬롯머신은 특정 금액을 걸고 손잡이를 당기면 판돈을 가져가거나 판돈의 두배를 되돌려 준다. 슬롯머신이 판돈을 가져가면 보상은 -1이 될것이고, 두배를 돌려주면 보상은 +1이 된다. 그중 하나가 다른 것보다 a+1 보상을 줄 확률이 높다고 가정한다. 게임을 1000번 했을 때 딸 수 있는 최대 금액을 얻으려면 어떤 전략을 사용해야할까?&lt;/p&gt;

&lt;p&gt;첫번째 전략은 최소로 게임을 했을 때 5개 슬롯머신 중 어느 슬롯멋니이 보상으로 1을 줄 확률이 가장 높은지 파악하는 것이다. 즉 성공률이 가장 높은 슬롯머신을 &lt;strong&gt;빨리&lt;/strong&gt; 찾아야 한다. 그 후 이 슬롯머신에서 계속 게임을 하면 된다.&lt;/p&gt;

&lt;p&gt;가장 어려운 부분은 &lt;strong&gt;최소한의 시도&lt;/strong&gt;로 최고의 슬롯머신을 찾아내야 한다.&lt;/p&gt;

&lt;h3 id=&quot;톰슨-샘플링-모델&quot;&gt;톰슨 샘플링 모델&lt;/h3&gt;

&lt;p&gt;톰슨 샘플링이 2개의 인수를 취하는 베타분포함수를 사용한다고 하자. 첫번째 인수가 높을수록 슬롯머신이 더 좋고 두번째 인수가 높을수록 슬롯머신이 더 나빠진다고 가정하자. 이 함수를 정의하면,&lt;/p&gt;

&lt;center&gt;$x=\beta(a,b)$&lt;/center&gt;

&lt;p&gt;이다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;conversionRates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.04&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conversionRates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;다섯개의 슬롯머신의 승률이 있다. 그후 샘플 수 N을 생성한다.
다음으로 각 샘플마다 모든 슬롯머신에 대한 승패의 집합을 정의한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conversionRates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;데이터셋 X의 N개 샘플 중 하나가 [0, 1, 0, 0, 1]이라고 한다면 2번이나 5번 슬롯머신에서 게임을 하면 된다.
다음으로 슬롯머신마다 게임한 결과 승패의 수를 셀 두 개의 배열을 생성하고 이긴횟수와 진횟수로 이름을 부여한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;nPosReward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nNegReward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;다음으로 데이터셋의 샘플마다 반복할 for 루프를 초기화하고 가장 좋은 슬롯머신을 선택한다. 그후 베타분포에서 난수를 취해 전체 슬롯머신에서 가장 높은 값을 찾는다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;selected&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;maxRandom&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;randomBeta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nPosReward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nNegReward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randomBeta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxRandom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;maxRandom&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randomBeta&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;selected&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;
            
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;selected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nPosReward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;selected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nNegReward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;selected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;nSelected&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nPosReward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nNegReward&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Machine number '&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;' ws selected '&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nSelected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;' times'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Best machine is '&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nSelected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Machine number 1 ws selected 9398.0 times
Machine number 2 ws selected 136.0 times
Machine number 3 ws selected 318.0 times
Machine number 4 ws selected 103.0 times
Machine number 5 ws selected 45.0 times
Best machine is 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이 문제를 해결할 때 처음으로 할 일은 다섯대의 슬롯머신을 차례로 시도하는것이다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;라운드&lt;/th&gt;
      &lt;th&gt;Machine Number&lt;/th&gt;
      &lt;th&gt;보상&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;+1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;+1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;라고 가정하면,&lt;/p&gt;

&lt;p&gt;각 슬롯머신에 대해 두 개의 새 변수를 도입한다. 하나는 슬롯머신이 보상으로 0을 반환한 횟수를 세고 다른 변수는 그 슬롯머신이 보상으로 1을 반환한 횟수를 센다.&lt;/p&gt;

&lt;p&gt;이 변수들을 $N_{i}^{0}(n)$과 $N_{i}^{1}(n)$으로 표기하며 여기에서 $N_{i}^{0}(n)$은 $i$번째 슬롯머신이 $n$번째 라운드까지 보상으로 0을 반환한 횟수이며, $N_{i}^{1}(n)$은 $i$번째 슬롬섯니이 $n$번째 라운드까지 보상으로 1을 반환한 횟수이다. 구체적으로 살펴보면&lt;/p&gt;

&lt;p&gt;$N_{1}^{0}(1) = 1$은 슬롯머신 1이 1라운드에 1패를 반환함을 뜻한다.&lt;/p&gt;

&lt;p&gt;$N_{1}^{0}(1) = 0$은 슬롯머신 1이 1라운드에 0승을 반환함을 뜻한다.&lt;/p&gt;

&lt;p&gt;$N_{5}^{1}(4) = 1$은 슬롯머신 5가 4라운드에 1승을 반환함을 뜻한다.&lt;/p&gt;

&lt;h3 id=&quot;분포&quot;&gt;분포&lt;/h3&gt;

&lt;p&gt;변수의 분포는 변수가 취할 수 있는 가능한 값 범위의 각 값에 대해 이 변수가 해당 값과 같을 확률을 제공하는 함수다.
매 회 게임에서 각 슬롯머신을 특정 베터 분포와 연결시키면&lt;/p&gt;

&lt;center&gt;$\beta(N_{i}^{1}(n)+1, N_{i}^{0}(n)+1)$&lt;/center&gt;

&lt;p&gt;베타분포 $\beta(a,b)$에서 매개변수 a가 클수록 분포가 오른쪽으로 더 많이 이동한다. 매개변수 $b$가 클수록 분포는 왼쪽으로 더 많이 이동한다. 따라서 매 게임 $n$마다 각 슬롯머신에 대해 매개변수 $a$는 $n$번째 게임까지 그 슬롯머신이 1을 반환한 횟수(+1)이고 매개변수 $b$는 $n$번째 게임까지 그 슬롯머신이 0을 반환할 횟수(+1)이다.
정리하면 슬롯머신이 1(성공)을 더 많이 반환하면 분포가 오른쪽으로 더 많이 이동하고, 슬롯머신이 0(실패)을 더 많이 반환하면 분포가 왼쪽으로 더 많이 이동한다.&lt;/p&gt;

&lt;h3 id=&quot;톰슨-샘플링-전략&quot;&gt;톰슨 샘플링 전략&lt;/h3&gt;
&lt;p&gt;처음 다섯 번의 게임에서 다섯대의 슬롯머신 각각으로 게임한 다음 매 게임 $n$마다 AI가 할 일은&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;각 슬롯머신 $i(i=1,2,3,4,5)$에 대해 해당 슬롯머신의 베타 분포에서 무작위 추첨한 값 $\theta_{i}(n)$을 취한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;center&gt;$\theta_{i}(n) \sim \beta(N_{i}^{1}(n)+1, N_{i}^{0}(n)+1)$&lt;/center&gt;

&lt;ol&gt;
  &lt;li&gt;가장 높은ㅇ 샘플링된 값 $\theta_{i}(n)$을 가진 슬롯머신 $s(n)$에서 게임한다&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;$s(n) = argmax_{i=1,2,3,4,5}(\theta_{i}(n))$&lt;/center&gt;

&lt;ol&gt;
  &lt;li&gt;$N_{s(n)}^{i}(n)$ 이나 $N_{s(n)}^{0}(n)$을 업데이트한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;게임했던 슬롯머신 $s(n)$이 보상으로 1을 반환하면,&lt;/p&gt;

&lt;center&gt;$N_{s(n)}^{1}(n) : = N_{s(n)}^{1}(n) + 1$&lt;/center&gt;

&lt;p&gt;게임했던 슬롯머신 $s(n)$이 보상으로 0을 반환하면,&lt;/p&gt;

&lt;center&gt;$N_{s(n)}^{0}(n) : = N_{s(n)}^{0}(n) + 1$&lt;/center&gt;

&lt;p&gt;그 다음 모든 돈을 쓸때까지 매 게임에서 이 세단계를 반복한다. 이 전략은 톰슨 샘플링이라 하며 기본적이지만 강력한 모델이다.&lt;/p&gt;

&lt;h3 id=&quot;톰슨-샘플링-마무리&quot;&gt;톰슨 샘플링 마무리&lt;/h3&gt;
&lt;p&gt;각 슬롯머신에는 자체 베타 분포가 있다. 게임이 진행되는 동안 전환율이 가장 높은 해당 슬롯머신의 베타 분포는 점차 오른쪽으로 이동하고 전환율이 낮은 전략의 베타 분포는 점차 왼쪽으로 이동한다. 따라서 이러한 단계를 반복해 전환율이 가장 높은 슬롯머신이 점점 더 많이 선택된다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;참고 : 아들랑 드 폰테베 &lt;strong&gt;『&lt;/strong&gt;강화학습/심층강화학습특강&lt;strong&gt;』&lt;/strong&gt;, 위키북스(2021), p38-54&lt;/p&gt;
&lt;/blockquote&gt;</content>

      
      
      
      
      

      <author>
          <name>ilvnax24er</name>
        
        
      </author>

      

      
        <category term="reinforcement" />
      

      
        <summary type="html">첫 AI 모델 만들기 - Slot machine</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Reinforcement Learning - Intro</title>
      <link href="https://ilvnax24er.github.io/1reinforcement-intro" rel="alternate" type="text/html" title="Reinforcement Learning - Intro" />
      <published>2021-07-05T00:00:00+09:00</published>
      <updated>2021-07-05T00:00:00+09:00</updated>
      <id>https://ilvnax24er.github.io/1reinforcement-intro</id>
      <content type="html" xml:base="https://ilvnax24er.github.io/1reinforcement-intro">&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;h3 id=&quot;강화학습이란&quot;&gt;강화학습이란?&lt;/h3&gt;

&lt;p&gt;AI의 한 형태지만 환경과 상호작용하는 절차가 AI 관점에서 재현되고 인간 지능을 모방하려는 형태의 인공지능.&lt;/p&gt;

&lt;h3 id=&quot;5가지-원칙&quot;&gt;5가지 원칙&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;입출력 시스템&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;모든 AI 모델은 입력과 출력의 보편적인 원칠에 기반한다. 강화학습에서 입력은 state(상태) 또는 input state(입력 상태)라고 부른다. 출력은 AI가 수행하는 action(행동)이다. 그 사이에는 state를 input으로 취하고 action을 output으로 반환하는 함수만 있다. 그 함수는 policy(정책)이라고 한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;보상&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;모든 AI는 보상 시스템에 의해 그 성과가 측정된다. 보상은 단순히 AI가 시간이 지남에 따라 얼마나 잘하는지 알려주는 지표이다. AI의 궁극적인 목표는 시간이 지남에 따라 누적된 보상을 극대화하는 것이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;AI 환경&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;AI환경은 일정시간(t)마다 세가지를 정의하는 매우 단순한 프레임 워크이다. 입력(state), 출력(action), 보상으로 구성된 환경을 정의한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;마르코프 결정 프로세스&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;마르코프 결정 프로세스는 시간이 지남에 따라 AI가 환경과 어떻게 상호작용하는지를 모델링하는 프로세스이다. 이 프로세스는 t=0에서 시작해 반복할 때마다 동일한 형태의 전이(transition)과정을 따른다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;AI는 현재 상태 $s_{t}$를 관측한다.&lt;/li&gt;
  &lt;li&gt;AI는 행동 $a_{t}$를 수행한다.&lt;/li&gt;
  &lt;li&gt;AI는 보상 $r_{t} = R(s_{t}, a_{t})$을 받는다.&lt;/li&gt;
  &lt;li&gt;AI는 다음 상태 $S_{t+1}$에 들어간다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;강화학습에서 AI의 목적은 $r_{t} = R(s_{t}, a_{t})$의 합을 극대화하는 것이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;훈련과 추론&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;훈련모드&lt;/p&gt;

    &lt;p&gt;AI가 훈련되는 기간을 훈련모드라고 한다. 그 기간동안 AI는 성공할 때까지 특정 목표를 반복해서 달성하려고 한다. 각 시도 후에는 다음 시도에서 더 잘 수행하도록 AI 모델의 매개변수가 수정된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;추론모드&lt;/p&gt;

    &lt;p&gt;AI가 완전히 훈련되고 잘 수행할 준비가 된 후에 뒤따른다. AI가 훈련모드에서 이전에 달성하도록 훈련된 목표를 달성하기 위한 행동을 함으로써 환경과 상호작용하는것으로 구성된다. 추론모드에서는 각 에피소드가 끝날 때 매개변수가 조정되지 않는다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;참고 : 아들랑 드 폰테베 &lt;strong&gt;『&lt;/strong&gt;강화학습/심층강화학습특강&lt;strong&gt;』&lt;/strong&gt;, 위키북스(2021), p30-37&lt;/p&gt;
&lt;/blockquote&gt;</content>

      
      
      
      
      

      <author>
          <name>ilvnax24er</name>
        
        
      </author>

      

      
        <category term="reinforcement" />
      

      
        <summary type="html">Intro</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Reinforcement Learning - Sales and Advertisement</title>
      <link href="https://ilvnax24er.github.io/3sales-adv" rel="alternate" type="text/html" title="Reinforcement Learning - Sales and Advertisement" />
      <published>2021-07-05T00:00:00+09:00</published>
      <updated>2021-07-05T00:00:00+09:00</updated>
      <id>https://ilvnax24er.github.io/3sales-adv</id>
      <content type="html" xml:base="https://ilvnax24er.github.io/3sales-adv">&lt;h2 id=&quot;문제정의&quot;&gt;문제정의&lt;/h2&gt;

&lt;p&gt;고객에게 가격할인, 회원 딜과 같은 프리미엄 회원제에 가입할 수 있는 옵션을 제공해 할인된 가격, 특가 상품 등과 같은 혜택을 제공한다. 구독할 수 있는 옵션을 제공하는것이 포함되어 있다. 연간 200달러의 가격에 제공되며, 최대한 많은 고객을 프리미엄 회원제에 가입시키는 것이다. 비즈니스 수익을 극대화하기 위해 AI를 구축한다.
1억명의 고객이 있다고 가정하며 프리미엄 회원으로 전환하도록 유도하는 전환율을 가진 전략이 존재한다.&lt;/p&gt;

&lt;h3 id=&quot;시뮬레이션-내부에-환경-구축하기&quot;&gt;시뮬레이션 내부에 환경 구축하기&lt;/h3&gt;
&lt;p&gt;고객이 프리미엄 회원제에 가입했다면 보상은 1, 그렇지 않은 경우 보상은 0이다. 이것을 정리하면&lt;/p&gt;

&lt;p&gt;1라운드 : 고객 1에게 전략 1의 광고 1을 표시하고 고객이 가입을 결정하는지를 확인한다. 고객이 가입하면 보상으로 1을 받고 그렇지 않으면 봇아으로 0을 받는다. 보상을 수집하면 다음 고객(라운드)으로 넘어간다.&lt;/p&gt;

&lt;p&gt;2라운드 : 새로운 고객, 고객 2에게 전략 2의 광로 2를 표시하고 고객이 가입하는지 확인한다. 가이바면 보상으로 1을 받고 그렇지 않으면 보상으로 0을 받는다. 보상을 수집하면 다음 고객으로 넘어간다.&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;10라운드 : 톰슨 샘플링을 통해 어느 광고가 가장 많은 고객을 프리미엄 회원제 가입으로 전환시킬 수 있는 능력이 가장 강한지 알려준다. 우리의 목표는 이 추가 수익을 얻는 데 있다. 톰슨 샘플링으로 결정된 AI는 새로운 고객, 고객 10에게 9가지 광고 중 하나를 선택해 보여주고 가입했는지 확인한다. 가입하면 보상으로 1을 받고 그렇지 않으면 0을 받는다. 보상을 수집하면 다음 고객으로 넘어간다.&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;이 과정을 반복해 AI가 가장 높은 전환율을 보이는 최상의 광고를 찾을 때까지 반복한다.&lt;/p&gt;

&lt;p&gt;다음의 전환율을 가정하면&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;전략&lt;/th&gt;
      &lt;th&gt;전환율&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.13&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.09&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;0.04&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0.20&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;0.08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0.01&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;가장 높은 전환율을 보이는 전략이 7번이라는것을 안다. 하지만 톰슨 샘플링은 이 사실을 모른다. 톰슨 샘플링은 이전 라운드까지 누적된 성공횟수와 실패 횟수만 알고있다.&lt;/p&gt;

&lt;h3 id=&quot;시뮬레이션-실행&quot;&gt;시뮬레이션 실행&lt;/h3&gt;
&lt;p&gt;예를들어 전환율이 0.16인 전략은 각 고객에 대해 0과 1사이의 무작위 수를 뽑는다. 이 무작위 수가 0에서 0.16 사이 값일 확률이 16%이며 0.16과 1사이 값일 확률은 84%이다. 따라서 무작위 수가 0과 0.16사이라면 보상으로 1을 얻고, 0.16에서 1사이라면 0을 얻는다. 이것은 회원제에 가입할 확률이 16%인 사실을 나타낸다.&lt;/p&gt;

&lt;h3 id=&quot;ai-solution&quot;&gt;AI Solution&lt;/h3&gt;
&lt;p&gt;10000번의 라운드에 걸쳐 각 라운드 $n$마다 다음 세 단계를 반복한다.&lt;/p&gt;

&lt;p&gt;1단계 : 각 전략 $i$에 대해 다음 분포에서 무작위 값을 뽑는다.&lt;/p&gt;
&lt;center&gt;$\theta_{i}(n) \sim \beta(N_{i}^{1}(n) + 1, N_{i}^{0}(n) + 1)$&lt;/center&gt;

&lt;p&gt;2단계 : 가장 높은 $\theta_{i}(n)$을 갖는 전략 $s(n)$을 선택한다.&lt;/p&gt;
&lt;center&gt;$s(n) = argmax_{i \subseteq \{1,\cdots,9\}}(\theta_{i}(n))$&lt;/center&gt;

&lt;p&gt;3단계 : 다음 조건에 따라 $N_{s(n)}^{1}(n)$과 $N_{s(n)}^{0}(n)$을 업데이트한다.&lt;/p&gt;

&lt;p&gt;선택된 전략 $s(n)$이 보상으로 1을 받으면 $N_{s(n)}^{1}(n) : = N_{s(n)}^{1}(n) + 1$&lt;/p&gt;

&lt;p&gt;선택된 전략 $s(n)$이 보상으로 0을 받으면 $N_{s(n)}^{0}(n) : = N_{s(n)}^{0}(n) + 1$&lt;/p&gt;

&lt;h2 id=&quot;구현&quot;&gt;구현&lt;/h2&gt;
&lt;h3 id=&quot;톰슨-샘플링-대-무작위-선택&quot;&gt;톰슨 샘플링 대 무작위 선택&lt;/h3&gt;
&lt;p&gt;톰슨 샘플링을 구현하면서 라운드마다 임의로 전략을 선택하는 무작위 선택 알고리즘도 구현한다. 이 알고리즘은 톰슨 샘플링 모델의 성능을 평가하기 위한 기준이 되며 동일한 환경 행렬에서 경쟁한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;성능지표
마지막으로 전체 시뮬레이션이 끝나면 다음 공식에 의해 정의된 상대 수익률을 계산해 톰슨샘플링의 성능을 평가할 수 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;상대 수익률 = $\frac{(톰슨 샘플링의 보상 총합) - (무작위 선택의 보상 총합)}{무작위 선택의 보상 총합}$&lt;/center&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 고객 수
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 전략 수
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;conversion_rates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.09&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.04&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.08&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conversion_rates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 라운드마다 무작위 선택 알고리즘에 의해 선택된 전략을 포함한 리스트.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strategies_selected_rs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 라운드마다 톰슨 샘플링 모델에 의해 선택된 전략을 포함한 리스트.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strategies_selected_ts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#무작위 선택 알고리즘에 의해 라운드가 반복될 때마다 누적된 보상의 총합
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_reward_rs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#톰슨 샘플링 모델에 의해 라운드가 반복될 때마다 누적된 보상의 총합
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_reward_ts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 9개의 요소로 이루어진 리스트로 각 전략이 보상으로 1을 받은 횟수를 포함하고 있다.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numbers_of_rewards_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 9개의 요소로 이루어진 리스트로 각 전략이 보상으로 0을 받을 횟수를 포함하고 있다.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numbers_of_rewards_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 무작위 선택
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;strategy_rs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;strategies_selected_rs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strategy_rs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;reward_rs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strategy_rs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;total_reward_rs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_reward_rs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward_rs&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# 톰슨 샘플링
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;strategy_ts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;max_random&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;random_beta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;betavariate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numbers_of_rewards_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                        &lt;span class=&quot;n&quot;&gt;numbers_of_rewards_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_beta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;max_random&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_beta&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;strategy_ts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;reward_ts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strategy_ts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward_ts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;numbers_of_rewards_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strategy_ts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numbers_of_rewards_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strategy_ts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;numbers_of_rewards_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strategy_ts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numbers_of_rewards_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strategy_ts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
            
    &lt;span class=&quot;n&quot;&gt;strategies_selected_ts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strategy_ts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;total_reward_ts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_reward_ts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward_ts&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;relative_return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_reward_ts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_reward_rs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_reward_rs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relative return: {:.0f} %'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relative_return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;relative return: 93 %
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strategies_selected_ts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Histogram of Selections'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'strategy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'selected number of times'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xticks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;\assets\built\images\post_images\2021-07-05-3sales-adv\sales-graph.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;참고 : 아들랑 드 폰테베 &lt;strong&gt;『&lt;/strong&gt;강화학습/심층강화학습특강&lt;strong&gt;』&lt;/strong&gt;, 위키북스(2021), p55-71&lt;/p&gt;
&lt;/blockquote&gt;</content>

      
      
      
      
      

      <author>
          <name>ilvnax24er</name>
        
        
      </author>

      

      
        <category term="reinforcement" />
      

      
        <summary type="html">문제정의</summary>
      

      
      
    </entry>
  
</feed>

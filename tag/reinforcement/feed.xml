<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://ilvnax24er.github.io/tag/reinforcement/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://ilvnax24er.github.io/" rel="alternate" type="text/html" />
  <updated>2021-07-06T15:39:07+09:00</updated>
  <id>https://ilvnax24er.github.io/tag/reinforcement/feed.xml</id>

  
  
  

  
    <title type="html">Archive | </title>
  

  
    <subtitle>Deeplearning Archive</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">Reinforcement - Intro</title>
      <link href="https://ilvnax24er.github.io/reinforcement-intro" rel="alternate" type="text/html" title="Reinforcement - Intro" />
      <published>2021-07-05T00:00:00+09:00</published>
      <updated>2021-07-05T00:00:00+09:00</updated>
      <id>https://ilvnax24er.github.io/reinforcement-intro</id>
      <content type="html" xml:base="https://ilvnax24er.github.io/reinforcement-intro">&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;h3 id=&quot;강화학습이란&quot;&gt;강화학습이란?&lt;/h3&gt;

&lt;p&gt;AI의 한 형태지만 환경과 상호작용하는 절차가 AI 관점에서 재현되고 인간 지능을 모방하려는 형태의 인공지능.&lt;/p&gt;

&lt;h3 id=&quot;5가지-원칙&quot;&gt;5가지 원칙&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;입출력 시스템&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;모든 AI 모델은 입력과 출력의 보편적인 원칠에 기반한다. 강화학습에서 입력은 state(상태) 또는 input state(입력 상태)라고 부른다. 출력은 AI가 수행하는 action(행동)이다. 그 사이에는 state를 input으로 취하고 action을 output으로 반환하는 함수만 있다. 그 함수는 policy(정책)이라고 한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;보상&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;모든 AI는 보상 시스템에 의해 그 성과가 측정된다. 보상은 단순히 AI가 시간이 지남에 따라 얼마나 잘하는지 알려주는 지표이다. AI의 궁극적인 목표는 시간이 지남에 따라 누적된 보상을 극대화하는 것이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;AI 환경&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;AI환경은 일정시간(t)마다 세가지를 정의하는 매우 단순한 프레임 워크이다. 입력(state), 출력(action), 보상으로 구성된 환경을 정의한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;마르코프 결정 프로세스&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;마르코프 결정 프로세스는 시간이 지남에 따라 AI가 환경과 어떻게 상호작용하는지를 모델링하는 프로세스이다. 이 프로세스는 t=0에서 시작해 반복할 때마다 동일한 형태의 전이(transition)과정을 따른다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;AI는 현재 상태 $s_{t}$를 관측한다.&lt;/li&gt;
  &lt;li&gt;AI는 행동 $a_{t}$를 수행한다.&lt;/li&gt;
  &lt;li&gt;AI는 보상 $r_{t} = R(s_{t}, a_{t})$을 받는다.&lt;/li&gt;
  &lt;li&gt;AI는 다음 상태 $S_{t+1}$에 들어간다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;강화학습에서 AI의 목적은 $r_{t} = R(s_{t}, a_{t})$의 합을 극대화하는 것이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;훈련과 추론&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;훈련모드&lt;/p&gt;

    &lt;p&gt;AI가 훈련되는 기간을 훈련모드라고 한다. 그 기간동안 AI는 성공할 때까지 특정 목표를 반복해서 달성하려고 한다. 각 시도 후에는 다음 시도에서 더 잘 수행하도록 AI 모델의 매개변수가 수정된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;추론모드&lt;/p&gt;

    &lt;p&gt;AI가 완전히 훈련되고 잘 수행할 준비가 된 후에 뒤따른다. AI가 훈련모드에서 이전에 달성하도록 훈련된 목표를 달성하기 위한 행동을 함으로써 환경과 상호작용하는것으로 구성된다. 추론모드에서는 각 에피소드가 끝날 때 매개변수가 조정되지 않는다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;참고 : 아들랑 드 폰테베 &lt;strong&gt;『&lt;/strong&gt;강화학습/심층강화학습특강&lt;strong&gt;』&lt;/strong&gt;, 위키북스(2021), p30-37&lt;/p&gt;
&lt;/blockquote&gt;</content>

      
      
      
      
      

      <author>
          <name>ilvnax24er</name>
        
        
      </author>

      

      
        <category term="reinforcement" />
      

      
        <summary type="html">Intro</summary>
      

      
      
    </entry>
  
</feed>

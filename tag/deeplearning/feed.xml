<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://ilvnax24er.github.io/tag/deeplearning/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://ilvnax24er.github.io/" rel="alternate" type="text/html" />
  <updated>2021-07-08T13:42:07+09:00</updated>
  <id>https://ilvnax24er.github.io/tag/deeplearning/feed.xml</id>

  
  
  

  
    <title type="html">Archive | </title>
  

  
    <subtitle>Deeplearning Archive</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">심층학습-Deep Feedforward Neural Network(4)</title>
      <link href="https://ilvnax24er.github.io/architecture-design" rel="alternate" type="text/html" title="심층학습-Deep Feedforward Neural Network(4)" />
      <published>2021-07-04T00:00:00+09:00</published>
      <updated>2021-07-04T00:00:00+09:00</updated>
      <id>https://ilvnax24er.github.io/architecture-design</id>
      <content type="html" xml:base="https://ilvnax24er.github.io/architecture-design">&lt;h2 id=&quot;architecture-설계&quot;&gt;Architecture 설계&lt;/h2&gt;

&lt;p&gt;신경망의 핵심 고려사항중 하나는 아키텍처의 선택이다. 단위 개수나 단위들의 연결방식 등을 포괄하는 신경망의 전반적인 구조를 뜻한다. 대부분의 신경망은 layer라고 부르는 단위들의 묶음으로 조직화된다. 각 층이 그 이전 층의 함수로서 작용하는 형태의 사슬구조로 조직화한다. 첫번째 층을 정의하면&lt;/p&gt;

&lt;center&gt;$h^{(1)} = g^{(1)}(W^{(1)T}x + b^{(1)})$&lt;/center&gt;

&lt;p&gt;두번째 층은&lt;/p&gt;

&lt;center&gt;$h^{(2)} = g^{(2)}(W^{(2)T}h^{(1)} + b^{(2)})$&lt;/center&gt;

&lt;p&gt;로 정의된다.&lt;/p&gt;

&lt;p&gt;이러한 사슬기반 아키텍처에서 신경망의 구조에 관한 주된 고려사항은 신경망의 깊이와 각 층의 너비를 선택하는 것이다. 층이 더 많은 신경망은 각 층의 단위와 매개변수가 훨씬 적고 시험 집합으로 일반화되는 경우가 많지만, 최적화가 어려운 경향이 있다.&lt;/p&gt;

&lt;h3 id=&quot;보편-근사-정리와-신경망의-깊이&quot;&gt;보편 근사 정리와 신경망의 깊이&lt;/h3&gt;

&lt;p&gt;보편근사정리(universal approximation theroem)에 따르면 선형 출력층이 있는, 그리고 임의의 squashing 활성화 함수(ex. 로그 S자형 활성화 함수)를 사용하는 은닉층이 적어도 하나이상 있는 순방향 신경망은 한 유한 차원 공간을 다른 유한차원 공간으로 사상하는 모든 보렐 가측함수(borel measurable function)를 우리가 원하는 정확도로 근사할 수 있다. 또한, 순방향 신경망의 미분들은 해당 함수의 미분들을 임의의 정확도로 잘 근사할 수 있다.(Hornik, 1990).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;보렐 가측성&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;$R^{n}$의 닫힌 유계 집합에 대한 모든 연속 함수는 보렐 가측함수이며, 순방향 신경망으로 근사할 수 있다.&lt;/p&gt;

&lt;p&gt;즉 보편근사정리는 학습하고자 하는 함수가 어떤 것이든 큰 MLP로 그 함수를 표현할 수 있음을 뜻한다. 그러나 훈련 알고리즘이 그 함수를 학습할 수 있다는 보장은 없다. MLP가 함수를 표현할 수 있어도 그 함수를 학습하지 못하는 이유는, 첫째로, 훈련에 쓰이는 최적화 알고리즘이 원하는 함수에 해당하는 매개변수 값들을 찾아내지 못할 수 있다. 둘째로, 과대적합 때문에 훈련 알고리즘이 엉뚱한 함수를 선택할 수 있다.&lt;/p&gt;

&lt;p&gt;보편 근사 정리에 따르면, 주어진 함수를 우리가 원하는 정확도로 근사하기에 충분한 크기의 신경망이 존재한다. 그러나 그러한 신경망이 얼마나 큰지를 정리가 말해 주지는 않는다.&lt;/p&gt;

&lt;p&gt;단층 순방향 신경망은 임의의 함수를 표현하기에 충분하지만, 그 층이 감당할 수 없을 정도로 크고 학습과 일반화가 제대로 일어나지 않을 가능성이 있다. 더 깊은 모형을 사용하면 원하는 함수를 표현하는 데 필요한 단위 개수가 줄어들고 일반화 오차의 양도 줄어든다.&lt;/p&gt;

&lt;p&gt;입력이 $d$개이고 깊이가 $l$, 은닉층 당 단위가 $n$개인 심층 정류 신경망으로 얻을 수 있는 선형 영역들의 개수는&lt;/p&gt;

&lt;center&gt;$\begin{pmatrix}
\binom{n}{k}^{d(l-1)}
&amp;amp; n^{d}
\end{pmatrix}$&lt;/center&gt;

&lt;p&gt;즉 깊이 $l$이 지수인 거듭제곱 규모이다. 단위당 필터가 $k$개인 맥스아웃 신경망의 경우 선형 영역의 개수는&lt;/p&gt;

&lt;center&gt;$O(k^{l-1)+d})$&lt;/center&gt;

&lt;p&gt;신경망 아키텍처 중에는 특정 과제에 맞게 개발된것들이 있다. CNN의 경우 Computer Vision에 RNN은 순방향 신경망을 순차열 처리에 맞게 일반화 한것이 예이다. 일반적으로 신경망의 층들이 반드시 사슬로 연결될 필요는 없다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>ilvnax24er</name>
        
        
      </author>

      

      
        <category term="deeplearning" />
      

      
        <summary type="html">Architecture 설계</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Recurrent Neural Network</title>
      <link href="https://ilvnax24er.github.io/RNN" rel="alternate" type="text/html" title="Recurrent Neural Network" />
      <published>2021-07-01T00:00:00+09:00</published>
      <updated>2021-07-01T00:00:00+09:00</updated>
      <id>https://ilvnax24er.github.io/RNN</id>
      <content type="html" xml:base="https://ilvnax24er.github.io/RNN">&lt;h1 id=&quot;recurrent-neural-network&quot;&gt;Recurrent Neural Network&lt;/h1&gt;

&lt;h3 id=&quot;intro&quot;&gt;Intro&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;참고 : 이안 굿펠로, 요수아 벤지오, 에런 쿠빌, &lt;strong&gt;『&lt;/strong&gt;심층학습&lt;strong&gt;』&lt;/strong&gt;, Jpub(2018), p368-408&lt;/p&gt;
&lt;/blockquote&gt;</content>

      
      
      
      
      

      <author>
          <name>ilvnax24er</name>
        
        
      </author>

      

      
        <category term="deeplearning" />
      

      
        <summary type="html">Recurrent Neural Network</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Convolutional Neural Network</title>
      <link href="https://ilvnax24er.github.io/CNN" rel="alternate" type="text/html" title="Convolutional Neural Network" />
      <published>2021-06-30T00:00:00+09:00</published>
      <updated>2021-06-30T00:00:00+09:00</updated>
      <id>https://ilvnax24er.github.io/CNN</id>
      <content type="html" xml:base="https://ilvnax24er.github.io/CNN">&lt;h1 id=&quot;convolutional-neural-network&quot;&gt;Convolutional Neural Network&lt;/h1&gt;

&lt;h3 id=&quot;intro&quot;&gt;Intro&lt;/h3&gt;

&lt;p&gt;정해진 격자 형태로 배열된 자료를 처리하는데 특화된 신경망이다. 이 신경망이 합성곱(convolution)연산을 사용하기 때문에 붙은 이름이다. 적어도 하나의 층에서 일반적인 행렬 곱셈 대신 합성곱을 사용하는 신경망이면 합성곱 신경망이라고 할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;합성곱-연산&quot;&gt;합성곱 연산&lt;/h3&gt;

&lt;p&gt;합성곱은 실숫값을 받는 두 함수에 관한 연산이다.&lt;/p&gt;

&lt;p&gt;일반적으로 $s(t) = (x \ast \omega)(t)$ 로 표기한다.&lt;/p&gt;

&lt;p&gt;합성곱은 첫 인수를 input이라고 부르며, 두번째 인수를 kernel이라고 부른다. 그리고 합성곱의 출력을 feature map이라고 부른다.&lt;/p&gt;

&lt;p&gt;$x$와 $\omega$가 오직 정수 $t$에 대해서만 정의된다고 가정하고, 이산적인 합성곱 연산을 정의할 수 있다.&lt;/p&gt;

&lt;p&gt;$s(t) = (x \ast \omega)(t) = \sum_{a=-\infty}^{\infty} x(a)\omega(t-a)$&lt;/p&gt;

&lt;p&gt;종종 합성곱을 한 번에 여러 축에 적용할 때, 2차원 이미지 $I$를 입력으로 사용할 때는 다음과 같이 2차원 kernel $K$를 적용한다.&lt;/p&gt;

&lt;p&gt;$s(i,j) = (I \ast K)(i,j) = \sum_{m} \sum_{n} I(m,n)K(i-m,j-n)$&lt;/p&gt;

&lt;p&gt;이산 합성곱은 일부 성분들이 다른 성분들과 같아야 한다는 제약이 있는 행렬을 곱하는 연산에 해당한다. 단변량 이산 합성곱에서 행렬의 각 행은 반드시 그 위 행의 성분들을 한 자리 이동한 것과 같아야 한다. 이러한 조건을 만족하는 행렬을 퇴플리츠 행렬(Toeplitz matrix)라고 부른다. 2차원의 경우에는 double block circulant matrix가 합성곱에 해당한다.&lt;/p&gt;

&lt;p&gt;합성곱 신경망은 세가지 중요한 개념을 활용한다&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;sparse interaction, sparse weights, sparse connectivity&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;전통적인 신경망의 층들은 매개변수들의 행렬을 곱하는 연산을 수행한다. 이때 각 매개변수는 각 입력 단위와 각 출력 단위의 상호작용을 나타낸다. 결과적으로, 모든 출력 단위는 모든 입력단위와 상호작용한다. 그런데 보통의 경우 합성곱 신경망에는 sparse interaction이라는 성질이 있다. 이것은 kernel이 input보다 작기 때문에 생긴 성질인데, 윤곽선 같은 작고 의미 있는 특징들을 kernel로 검출할 수 있다. 따라서 상대적으로 적은 수의 매개변수들만 저장해도 되며, 결과적으로 효율성이 높아진다. 또한 출력을 계산하는데 필요한 연산의 수도 줄어든다.&lt;/p&gt;

&lt;p&gt;이처럼 희소한 상호작용만 서술하는 간단한 구성요소들 사이의 상호작용을 구축함으로써 여러 변수 사이의 복잡한 상호작용을 효율적으로 서술한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;parameter sharing&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;모형의 둘 이상의 함수에 같은 매개변수를 사용하는것을 말한다. 전통적인 신경망에서는 한 층의 출력을 계산할 때 가중치 행렬의 각 성분은 딱 한번만 사용된다. 합성곱 신경망에서 핵의 각 성분은 입력의 모든 곳에 쓰인다. 모든 위치에 대해 개별적인 매개변수 집합을 학습하는 것이 아니라 하나의 집합만 학습하는것에 해당한다. 이는 모형이 저장해야하는 매개변수 개수를 줄여 효율적이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;equivariance representation&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;합성곱의 경우, 매개변수 공유의 특정한 형태 때문에 신경망 층에는 이동에 대한 등변성이라는 성질이 생긴다. 함수 $f(x)$와 $g$가 있을 때 만일 $f(g(x)) = g(f(x))$ 이면 $f(x)$는 $g$에 대해 등변이다. 합송곱의 경우, 만일 $g$가 입력을 이동하는 함수이면 합성곱 함수는 $g$에 대해 등변이다. 즉 input에서 어떤 대상을 이동하면 output에서도 해당 표현이 같은 양으로 이동한다.&lt;/p&gt;

&lt;h3 id=&quot;pooling&quot;&gt;pooling&lt;/h3&gt;

&lt;p&gt;합성곱 신경망의 한 층은 세단계로 작동한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;다수의 합성곱을 병렬로 수행해서 선형 활성화 값들을 산출한다.&lt;/li&gt;
  &lt;li&gt;각 선형 활성화 값이 정류 선형 활성화 함수같은 비선형 활성화 함수를 거치게 된다. 이 단계를 detector stage라고 부른다.&lt;/li&gt;
  &lt;li&gt;pooling function을 이용해서 그 층의 출력을 좀 더 수정한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;pooling function은 특정 위치에서의 신경망의 출력을 근처 출력들의 요약통계량(summary statistics)으로 대체한다. pooling은 표현이 입력의 작은 이동에 대해 근사적으로 불변이 되게하는데 도움을 준다. 이동에 대한 불변성이란, 입력을 조금 이동해도 풀링된 출력들의 값들은 대부분 변하지 않는것을 말한다. &lt;strong&gt;어떤 특징의 구체적인 위치가 아니라 특징의 존재 여부 자체가 더 중요할 때는 국소 이동에 대한 불변성이 유용할 수 있다.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;참고 : 이안 굿펠로, 요수아 벤지오, 에런 쿠빌, &lt;strong&gt;『&lt;/strong&gt;심층학습&lt;strong&gt;』&lt;/strong&gt;, Jpub(2018), p368-408&lt;/p&gt;
&lt;/blockquote&gt;</content>

      
      
      
      
      

      <author>
          <name>ilvnax24er</name>
        
        
      </author>

      

      
        <category term="deeplearning" />
      

      
        <summary type="html">Convolutional Neural Network</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">심층학습-Deep Feedforward Neural Network(3)</title>
      <link href="https://ilvnax24er.github.io/deeplearning-hiddenunit" rel="alternate" type="text/html" title="심층학습-Deep Feedforward Neural Network(3)" />
      <published>2021-06-30T00:00:00+09:00</published>
      <updated>2021-06-30T00:00:00+09:00</updated>
      <id>https://ilvnax24er.github.io/deeplearning-hiddenunit</id>
      <content type="html" xml:base="https://ilvnax24er.github.io/deeplearning-hiddenunit">&lt;h3 id=&quot;hidden-unit&quot;&gt;Hidden Unit&lt;/h3&gt;

&lt;p&gt;출력단위 중에는 모든 입력 점에서 미분가능이 아닌것이 있다. 예를 들어 rectifeied linear unit $g(z) = \max{0,z}$는 $z =0$에서 미분이 불가능하다. 원칙적으로 $g$는 기울기 기반 학습에 사용할 수 없지만 비용함수의 극소점에 실제로 도달하기보다는 비용이 충분히 낮은 점에 도달할 때가 많이 때문에 경사 하강법이 충분히 잘 작동한다.&lt;/p&gt;

&lt;p&gt;대부분의 hidden unit은 하나의 벡터 $x$를 입력받아서 affine transformation $z = W^{T}x + b$를 계산하고, 성분별 비선형 함수 $g(z)$를 적용한다.&lt;/p&gt;

&lt;h4 id=&quot;relu&quot;&gt;ReLU&lt;/h4&gt;

&lt;p&gt;ReLU는 활성화 함수 $g(z) = \max{0, z}$를 사용한다. 선형 단위와 아주 비슷하기 때문에 최적화하기 쉽다. 차이점은, 정류 선형 단위는 정의역의 절반에 대해 0을 출력한다. 이덕분에 단위가 활성일 때면 항상 ReLU의 미분들이 큰 값을 유지한다. 이 기울기들은 크기가 클 뿐만 아니라 일치성 조건까지 만족한다. 이차도함수는 거의 모든 점에서 0으로 평가되며, 도함수는 단위가 활성화된 모든 점에서 1로 평가된다. 때문에 기울기 방향이 2차 효과들을 도입하는 활성화 함수의 경우보다 학습에 훨씬 유용하다.&lt;/p&gt;

&lt;center&gt;$h = g(W^{T}x + b)$&lt;/center&gt;

&lt;p&gt;매개변수들을 초기화할 때는 b의 모든 성분을 0.1 같은 작은 양수로 설정하는것이 바람직하다. 그러면 훈련 집합의 모든 입력에 대해 처음부터 활성화되어서 해당 미분들이 다음 층으로 전달될 가능성이 아주 커진다. ReLU의 단점은 활성화가 0인 견본들로는 기울기 기반 방법으로 학습할 수 없다는 것이다. 이러한 단점을 극복하고 모든 점에서 반드시 기울기가 산출되도록 개선한 일반화들이 존재한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;absolute value rectification&lt;/p&gt;
&lt;/blockquote&gt;

&lt;center&gt;$h_{i} = g(z,\alpha)_{i} = max(0, z_{i}) + \alpha_{i}min(o, z_{i})$&lt;/center&gt;
&lt;p&gt;$\alpha_{i} = -1$로 고정해서 $g(z) = |z|$가 되게 한다. 이 단위는 이미지에서 물체를 인식하는 신경망에 쓰인다. 입력 조명의 극성이 반전되어도 변하지 않는 특성들을 찾아야 하므로 절댓값 정류 단위가 적합하다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;leaky ReLU&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;$\alpha_{i}$를 0.01 같은 작은 값으로 고정한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;parametric ReLU, PReLU&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;$\alpha_{i}$를 학습 가능한 매개변수로 둔다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;maxout unit&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;입력 $z$를 $k$개의 값으로 이루어진 그룹들로 분할한다. 그렇다면 각 maxout단위는 각 그룹의 최대 성분을 출력한다.&lt;/p&gt;

&lt;center&gt;$g(z)_{i} = max_{j \subseteq G^{i}} z_{j}$&lt;/center&gt;

&lt;p&gt;maxout 단위들로 구성된 은닉층은 최대 k개인 조각별 선형 블록함수를 학습할 수 있다. 따라서 maxout 층은 단위들 사이의 관계를 학습하는것이 아니라 활성화 함수 자체를 학습하는 수단이다.&lt;/p&gt;

&lt;h4 id=&quot;tanh&quot;&gt;tanh&lt;/h4&gt;

&lt;center&gt;$g(z) = \tanh(z)$&lt;/center&gt;

&lt;p&gt;tanh는 0근처에서 항등함수와 비슷하다. 심층 신경망 $\hat{y} = \omega^{T} tanh (U^{T} tanh (V^{T} x))$의 훈련은 활성 값들을 작게 유지할 수만 있다면 선형 모형 $\hat{y} = \omega^{T}U^{T}V^{T}x$의 훈련과 비슷하다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;참고 : 이안 굿펠로, 요수아 벤지오, 에런 쿠빌, &lt;strong&gt;『&lt;/strong&gt;심층학습&lt;strong&gt;』&lt;/strong&gt;, Jpub(2018), p211-216&lt;/p&gt;
&lt;/blockquote&gt;</content>

      
      
      
      
      

      <author>
          <name>ilvnax24er</name>
        
        
      </author>

      

      
        <category term="deeplearning" />
      

      
        <summary type="html">Hidden Unit</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">심층학습-Deep Feedforward Neural Network(2)</title>
      <link href="https://ilvnax24er.github.io/gradient-decent" rel="alternate" type="text/html" title="심층학습-Deep Feedforward Neural Network(2)" />
      <published>2021-06-29T00:00:00+09:00</published>
      <updated>2021-06-29T00:00:00+09:00</updated>
      <id>https://ilvnax24er.github.io/gradient-decent</id>
      <content type="html" xml:base="https://ilvnax24er.github.io/gradient-decent">&lt;h3 id=&quot;기울기-기반-학습&quot;&gt;기울기 기반 학습&lt;/h3&gt;

&lt;p&gt;신경망은 비선형성이기 때문에 볼록함수를 손실함수로 사용하기가 적합하지 않ㅇ르 때가 많은것이 선형모형과 신경망의 가장 큰 차이점이다. 일반적으로 신경망에는 선형회귀 모형의 훈련에 쓰이는 연립방정식 해법이나 로지스틱 회귀 또는 SVM의 훈련에 쓰이는 전역 수렴을 보장하는 볼록함수 최적화 알고리즘 대신, 비용함수를 아주 낮은 값으로 이끄는 역할만 하는 반복적인 기울기 기반 최적화 절차를 사용한다. 볼록함수 최적화는 이론적으로 임의의 초기 매개변수들에서 시작해서 반드시 수렴한다. 비볼록 손실함수에 확률적 경사하강법을 적용할 때는 그러한 수렴이 보장되지 않으며, 결과가 초기 매개변수들에 민감하게 변한다. 순방향 신경망에서는 모든 가중치를 작은 난수들로 초기화하는 것이 중요하다.&lt;/p&gt;

&lt;h4 id=&quot;cost-function&quot;&gt;Cost Function&lt;/h4&gt;
&lt;p&gt;Cost function의 선택은 심층 신경망 설계의 매우 중요한 측면 중 하나이다. 대부분의 경우 매개변수적 모형은 하나의 분포 $p(y|x;\theta)$를 정의한다. 그리고 최대가능도 원리를 적용해서 훈련을 진행한다. 이 경우 훈련 자료와 모형의 예측 사이의 교차 엔트로피를 비용함수로 사용하면 된다.
하지만 $y$에 관한 전체 확률 분포를 예측하는 대신 $x$를 조건으로 한 $y$의 어떤 통계량을 예측하는 좀 더 단순한 접근 방식이 바람직할 때도 있다. 이 경우, 특화된 손실함수를 이용하면 추정량을 예측하는 모형을 훈련할 수 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;최대가능도를 이용한 조건부 확률 학습&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;대부분의 신경망은 최대가능도를 사용해서 훈련한다. 이는 비용함수가 음의 로그가능도라는 뜻인데, 훈련 자료와 모형 분포 사이의 교차 엔트로피로도 동등하게 서술할 수 있다. 이 경우 비용함수는&lt;/p&gt;
&lt;center&gt;$J(\theta) = -E_{x,y \sim \hat{p}자료}\log p_{모형}(y|x)$&lt;/center&gt;
&lt;p&gt;이다. 
비용함수의 구체적인 형태는 모형마다 다르며, 특히 $\log p_{모형}$의 구체적인 형태에 의존한다. 최대가능도에서 비용함수를 유도하는 방식의 장점은, 모형마다 매번 비용함수를 설계하는 부담이 없다는 것이다. 모형 $p(y|x)$를 결정하기만 하면 비용함수 $\log p(y|x)$가 자동으로 결정된다. 
최대가능도 추정을 수행하는 데 쓰이는 교차 엔트로피 비용함수의 한 가지 독특한 성질은, 실제 응용에 흔히 쓰이는 모형들에서 이 함수에 최솟값이 없을 때가 많다는 것이다. 이산 출력 변수의 경우, 대부분의 모형은 정확히 확률값 0과 1을 표현하지 못하며 그 두 극단에 얼마든지 가까워질 수 있는 방식으로 매개변수화 된다. 실숫값 출력 변수의 경우에는, 만일 모형이 출력 분포의 밀도를 제어할 수 있다면, 정확한 훈련 집합 출력들에 극도로 높은 밀도를 배정할 여지가 생간다. 그러면 교차 엔트로피는 음의 무한대에 접근하게 된다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;참고 : 이안 굿펠로, 요수아 벤지오, 에런 쿠빌, &lt;strong&gt;『&lt;/strong&gt;심층학습&lt;strong&gt;』&lt;/strong&gt;, Jpub(2018), p194-210&lt;/p&gt;
&lt;/blockquote&gt;</content>

      
      
      
      
      

      <author>
          <name>ilvnax24er</name>
        
        
      </author>

      

      
        <category term="deeplearning" />
      

      
        <summary type="html">기울기 기반 학습</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">심층학습-Deep Feedforward Neural Network(1)</title>
      <link href="https://ilvnax24er.github.io/deeplearning-tutorial" rel="alternate" type="text/html" title="심층학습-Deep Feedforward Neural Network(1)" />
      <published>2021-06-28T00:00:00+09:00</published>
      <updated>2021-06-28T00:00:00+09:00</updated>
      <id>https://ilvnax24er.github.io/deeplearning-tutorial</id>
      <content type="html" xml:base="https://ilvnax24er.github.io/deeplearning-tutorial">&lt;h3 id=&quot;deep-feedforward-neural-network&quot;&gt;Deep Feedforward Neural Network&lt;/h3&gt;

&lt;p&gt;순방향 신경망의 목표는 어떤 함수 $f^{*}$를 근사하는 것이다.&lt;/p&gt;

&lt;p&gt;Classifier의 경우 function $y = f^{*}(x)$ 는 주어진 입력 $x$를 하나의 범주 또는 부류(class) $y$에 사상한다. 
순방향 신경망은 하나의 사상 $y = f(x;\theta)$를 정의하고,
실제 바탕 함수를 가장 잘 근사하는 매개변수 $\theta$의 값들을 학습한다.&lt;/p&gt;

&lt;p&gt;이 모형의 이름에 feedforward가 있는 이유는, 이 모형에서 정보가 앞쪽으로만 흘러가기 때문이다. 정보는 $x$를 입력으로 하여 평가되는 함수를 통과한 후 $f$를 정의하는데 쓰이는 중간 계산들을 거쳐서 최종적으로 출력 $y$에 도달한다. 이 모형에는 모형의 출력이 다시 모형 자신의 입력으로 투입되는 &lt;strong&gt;feedback&lt;/strong&gt;이 전혀 없다.&lt;/p&gt;

&lt;p&gt;순방향 신경망의 Network는 이 모형을 흔히 서로 다른 함수들이 그물처럼 엮인 형태로 표현한다는 점을 반영한 것이다. 자료 구조의 관점에서 이 모형은 함수들의 연결 관계를 표현하는 direct acyclic graph에 해당한다. 예를 들어 세 함수 $f^{1}, f^{2}, f^{3}$을 연쇄적으로 적용해서 $f(x) = f^{3}(f^{2}(f^{1}(x)))$라는 하나의 사슬 구조를 만들 수 있다.&lt;/p&gt;

&lt;p&gt;이때 $f^{1}$을 신경망의 first layer, $f^{2}$를 second layer, 등으로 부른다. 사슬의 전체 길이는 모형의 depth에 해당한다. 순방향 신경망의 마지막 층을 출력층(output layer)라 부르며 신경망을 훈련한다는 것은 $f(x)$ 를 $f^{*}(x)$에 적합시키는 것에 해당한다.&lt;/p&gt;

&lt;p&gt;$f^{*}$의 가장 좋은 근사값이 산출되도록 layer를 운용하는 방법은 학습 알고리즘이 결정해야한다. 그런 layer들의 바람직한 출력을 훈련 자료가 보여주지 않는다는 점에서, 그런 층들을 hidden layer라고 부른다.&lt;/p&gt;

&lt;p&gt;일반적으로 순방향 신경망의 각 vector를 입력받아서 vector를 출력하는 하나의 vector값 함수라 할 수 있다. 이러한 vector의 차원은 신경망의 width를 결정한다.&lt;/p&gt;

&lt;p&gt;선형 모형을 $x$의 비선형 함수들로 확장하는 한 방법은, 선형 모형을 $x$ 자체가 아니라 변환된 입력 $\phi(x)$에 적용하는 것이다. 여기서 $\phi$는 하나의 비선형 변환이다. 문제는 어떠한 $\phi$를 사용할 것인가다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;무한차원 $\phi$ 같은 아주 일반적인 $\phi$를 사용하는것이다. $\phi(x)$의 차원이 충분히 높으면, 모형은 항상 훈련 집합에 적합하기에 충분한 수용력을 가지게 된다. 그러나 시험집합으로의 일반화는 여전히 나쁠 때가 많다. 아주 일반적인 특징 사상들은 대체로 국소 불변성 원리에만 기초할 뿐, 고급 문제를 풀기에 충분한 정도의 사전 정보를 부호화하지 않을 때가 많다.&lt;/li&gt;
  &lt;li&gt;좀 더 특화된 $\phi$를 사람이 직접 고안하는 것이다. 심층학습이 등장하기 전에는 이것이 주도적인 접근 방식이었다. 이를 위해서는 개별 과제마다 사람이 수많은 노력을 통해 과제의 성격에 따라 음성인식이나 컴퓨터 시각 같은 서로 다른 응용 역역에 특화된 노력이 필요했다.&lt;/li&gt;
  &lt;li&gt;심층 학습이 사용하는 전략은 $\phi$를 배우는 것이다. 이 접근 방식에서 모형은 $y=f(x;\theta, \omega) = $\phi$(x;\theta)^{T}\omega$이다. 이는 심층 순방향 신경망의 한 예로, $\phi$는 하나의 은닉층을 정의한다. 이 접근방식은 훈련문제에 &lt;strong&gt;convexity&lt;/strong&gt;가 있으면 실패하는 방식이다. 하지만 이 접근방식은 모형의 표현을 $\phi(x;\theta)$로 매개변수화하고 최적화 알고리즘을 이용해서 좋은 표현에 해당하는 $\phi$를 구한다. 필요하다면 첫 접근 방식의 장점(고도의 일반성)을 도입해서 수용력을 높일 수도 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;순방향 신경망을 설계하려면 hidden layer의 값을 계산하는 데 사용할 activation function을 선택해야 한다. 또한 신경망의 architecture도 설계해야 한다. 이를 위한 결정 사항으로는 layer의 개수나 그 layer들의 상호 연결 방식, 각 layer의 계산 단위 개수등이 있다.&lt;/p&gt;

&lt;h3 id=&quot;xor-학습&quot;&gt;XOR 학습&lt;/h3&gt;

&lt;p&gt;XOR(exclusive or)함수는 두 이진수 $x_{1}$과 $x_{2}$에 대한 연산이다. 두 이진수 중 정확히 하나가 1과 같으면 XOR 함수는 1을 돌려주고, 그렇지 않으면 0을 돌려준다.
이 XOR함수가 바로 이 예제에서 학습할 목표함수 $y=f^{*}(x)$에 해당한다.&lt;/p&gt;

&lt;p&gt;이 예제의 모형은 함수 $y=f(x;\theta)$를 제공하며, 학습 알고리즘은 $f$가 $f^{*}$와 최대한 비슷해지도록 매개변수 $\theta$를 적합시킨다.&lt;/p&gt;

&lt;p&gt;우리의 목표는, 신경망이 네 자료점 $X = {[0, 0]^{T},[0,1]^{T},[1,0]^{T},[1,1]^{T}}$에 대해 제대로 작동하게 만드는 것이다. 이를 위해 네 점 모두로 신경망을 훈련한다.
이 문제를 하나의 회귀 문제로 간주해서 MSE를 손실함수로 사용하는 것이다.&lt;/p&gt;

&lt;p&gt;MSE = $J(\theta) = \frac{1}{4} \sum_{x \subseteq X} (f^{*}(x) - f(x;\theta))^{2}$&lt;/p&gt;

&lt;p&gt;다음으로 모형 $f(x;\theta)$의 구체적인 형식을 선택해야 한다. 모형이 선형이고 $\theta$가 $\omega$와 $b$로 구성하면 모형은 다음과 같이 정의된다.
$f(x;\omega ,b) = x^{T}\omega + b$&lt;/p&gt;

&lt;p&gt;이렇게 하면 $J(\theta)$를 표준방정식을 이용해서 $\omega$와 $b$에 대한 닫힌 형식으로 최소화할 수 있다. 표준방정식을 풀면 $\omega = 0$ 과 $b = \frac{1}{2}$이 나온다. 즉 선형 모형은 그냥 모든 점에서 0.5를 출력한다. 선형 모형을 원래의 입력에 직접 적용하면 XOR 함수를 배우지 못한다. $x_{1} = 0$일 떄는
$x_{2}$가 증가함에 따라 모형의 출력도 반드시 증가해야 하고,
$x_{1} = 1$일 때는
$x_{2}$가 증가함에 따라 모형의 출력이 반드시 감소해야 한다. 선형 모형은
$x_{2}$에 항상 고정된 계수
$\omega_{2}$를 적용할 뿐,
$x_{1}$의 값으로
$x_{2}$의 계수를 변경하지는 못하기 때문이 이 문제를 풀 수 없다.&lt;/p&gt;

&lt;p&gt;은닉 단위가 두 개인 은닉층 하나가 있는 간단한 순방향 신경망으로 이 문제를 해결할 수 있다. 이 순방향 신경망의 은닉층은 은닉 단위들을 담은 벡터 $h$로 구성된다. 신경망의 제 1층인 은닉층은 함수$f^{1}(x;W ,c)$를 계산하며, 그 출력이 제2층인 출력층에 전달된다. 출력층은 이전처럼 그냥 하나의 선형회귀 모형인데, $x$가 아니라 $h$에 대해 작용한다는 것이 이전모형과 다른 점이다. 이러한 층들로 구성된 신경망은 두 함수 $h = f^{1}(x;W,c)$와 $y=f^{2}(h;\omega ,b)$를 사슬로 연결한 형태이다. 즉, 최종 모형은 $f(x;W,c,\omega ,b) = f^{2}(f^{1}(x))$이다.&lt;/p&gt;

&lt;p&gt;그런데 어떤 함수를 $f^{1}$로 두어야 할까? 지금까지는 선형 모형들이 잘 작동했으므로 $f^{1}$도 선형으로 두면 될 것 같다. 그러나 $f^{1}$이 선형이면 순방향 신경망 전체가 주어진 입력의 선형 함수가 된다. $f^{1}(x) = W^{T}x$이고 $f^{2}(h) = h^{T}\omega$이면 $f(x) = \omega^{T}W^{T}x$이다. $\omega^{‘} = W\omega$로 두면 이 함수를 $f(x) = x^{T}\omega^{‘}$으로 표현할 수 있는데, 이는 선형이다.&lt;/p&gt;

&lt;p&gt;완전한 신경망을 표현하면&lt;/p&gt;

&lt;p&gt;$f(x;W.c.\omega,b) = \omega^{T}max{0,W^{T}x+c}+b$ 이다.&lt;/p&gt;

&lt;p&gt;이제 XOR 문제를 풀어보면,&lt;/p&gt;

&lt;center&gt;$W = \begin{bmatrix}
1&amp;amp;1 \\\
1&amp;amp;1
\end{bmatrix}$&lt;/center&gt;

&lt;center&gt;$c = \begin{bmatrix}
0 \\\
-1
\end{bmatrix}$&lt;/center&gt;

&lt;center&gt;$\omega = \begin{bmatrix}
1 \\\
-2
\end{bmatrix}$&lt;/center&gt;

&lt;center&gt;$b=0$&lt;/center&gt;

&lt;p&gt;다음으로, 모형이 입력 전체를 처리하는 과정을 보면, 입력 $X$는 이진 입력 공간의 네 점 모두를 담은 설계 행렬로, 하나의 행은 하나의 견본에 해당한다.&lt;/p&gt;

&lt;center&gt;$X = \begin{bmatrix}
0&amp;amp;0\\\
0&amp;amp;1\\\
1&amp;amp;0\\\
1&amp;amp;1
\end{bmatrix}$&lt;/center&gt;

&lt;center&gt;$XW = \begin{bmatrix}
0&amp;amp;0\\\
1&amp;amp;1\\\
1&amp;amp;1\\\
2&amp;amp;2
\end{bmatrix}$&lt;/center&gt;

&lt;p&gt;여기에 vector $c$를 더하면,&lt;/p&gt;

&lt;center&gt;$XW = \begin{bmatrix}
0&amp;amp;-1\\
1&amp;amp;0\\
1&amp;amp;0\\
2&amp;amp;1
\end{bmatrix}$&lt;/center&gt;

&lt;p&gt;이 공간에서 모든 견본은 기울기가 1인 직선에 놓여있다. 그 직선을 따라갈 때 함수의 출력은 0에서 시작해서 1로 올라갔다가 다시 0으로 떨어진다. 선형 모형으로는 그런 함수를 구현할 수 없다. 각 견본에 대한 $h$의 값을 산출하기 위해, 지금까지의 결과에 다음과 같은 정류 선형 변환을 적용한다.&lt;/p&gt;

&lt;center&gt;$ \begin{bmatrix}
0&amp;amp;0\\\
1&amp;amp;0\\\
1&amp;amp;0\\\
2&amp;amp;1
\end{bmatrix}$&lt;/center&gt;

&lt;p&gt;이 변환을 적용하면 견본들 사이의 관계가 변한다. 이제는 견본들이 하나의 직선에 놓인 것이 아니라 선형으로 문제를 풀 수 있는 형태의 공간에 놓이게 된다.
마지막으로, 다음과 같은 가중치 벡트 $\omega$를 곱한다.&lt;/p&gt;

&lt;center&gt;$\begin{bmatrix}
0\\\
1\\\
1\\\
0
\end{bmatrix}$&lt;/center&gt;

&lt;p&gt;이렇게 하면 신경망은 입력 행렬의 모든 견본에 대해 정확한 답을 낸다.&lt;/p&gt;

&lt;p&gt;그러나 실제 응용에서는 모형 매개변수 개수와 훈련 견본 개수가 수십억 규모라서 지금처럼 해를 바로 도출하지 못할 수 있다.
그런 상황에서는 기울기 기반 최적화 알고리즘을 이용해서 오차가 아주 작은 매개변수들을 찾는 것이 한 방법이다.
XOR문제의 해는 손실함수의 한 전역 최소점에 해당하므로, 경사 하강법을 이용해서 그 점으로 수렴하는 것이 가능하다.
XOR문제의 다른 동등한 해들도 경사 하강법으로 구할 수 있다. 경사 하강의 수렴점은 매개변수들의 초깃값에 의존한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;참고 : 이안 굿펠로, 요수아 벤지오, 에런 쿠빌, &lt;strong&gt;『&lt;/strong&gt;심층학습&lt;strong&gt;』&lt;/strong&gt;, Jpub(2018), p185-194&lt;/p&gt;
&lt;/blockquote&gt;</content>

      
      
      
      
      

      <author>
          <name>ilvnax24er</name>
        
        
      </author>

      

      
        <category term="deeplearning" />
      

      
        <summary type="html">Deep Feedforward Neural Network</summary>
      

      
      
    </entry>
  
</feed>

<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://ilvnax24er.github.io/tag/deeplearning/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://ilvnax24er.github.io/" rel="alternate" type="text/html" />
  <updated>2021-07-03T02:19:16+09:00</updated>
  <id>https://ilvnax24er.github.io/tag/deeplearning/feed.xml</id>

  
  
  

  
    <title type="html">Archive | </title>
  

  
    <subtitle>Deeplearning Archive</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">Convolution Neural Network</title>
      <link href="https://ilvnax24er.github.io/deeplearning-tutorial" rel="alternate" type="text/html" title="Convolution Neural Network" />
      <published>2021-06-30T00:00:00+09:00</published>
      <updated>2021-06-30T00:00:00+09:00</updated>
      <id>https://ilvnax24er.github.io/deeplearning-tutorial</id>
      <content type="html" xml:base="https://ilvnax24er.github.io/deeplearning-tutorial">&lt;h1 id=&quot;convolution-neural-network&quot;&gt;Convolution Neural Network&lt;/h1&gt;</content>

      
      
      
      
      

      <author>
          <name>ilvnax24er</name>
        
        
      </author>

      

      
        <category term="deeplearning" />
      

      
        <summary type="html">Convolution Neural Network</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">심층학습-Deep Feedforward Neural Network(2)</title>
      <link href="https://ilvnax24er.github.io/gradient-decent" rel="alternate" type="text/html" title="심층학습-Deep Feedforward Neural Network(2)" />
      <published>2021-06-29T00:00:00+09:00</published>
      <updated>2021-06-29T00:00:00+09:00</updated>
      <id>https://ilvnax24er.github.io/gradient-decent</id>
      <content type="html" xml:base="https://ilvnax24er.github.io/gradient-decent">&lt;h3 id=&quot;기울기-기반-학습&quot;&gt;기울기 기반 학습&lt;/h3&gt;
&lt;p&gt;신경망은 비선형성이기 때문에 볼록함수를 손실함수로 사용하기가 적합하지 않ㅇ르 때가 많은것이 선형모형과 신경망의 가장 큰 차이점이다. 일반적으로 신경망에는 선형회귀 모형의 훈련에 쓰이는 연립방정식 해법이나 로지스틱 회귀 또는 SVM의 훈련에 쓰이는 전역 수렴을 보장하는 볼록함수 최적화 알고리즘 대신, 비용함수를 아주 낮은 값으로 이끄는 역할만 하는 반복적인 기울기 기반 최적화 절차를 사용한다. 볼록함수 최적화는 이론적으로 임의의 초기 매개변수들에서 시작해서 반드시 수렴한다. 비볼록 손실함수에 확률적 경사하강법을 적용할 때는 그러한 수렴이 보장되지 않으며, 결과가 초기 매개변수들에 민감하게 변한다. 순방향 신경망에서는 모든 가중치를 작은 난수들로 초기화하는 것이 중요하다.&lt;/p&gt;

&lt;h4 id=&quot;cost-function&quot;&gt;Cost Function&lt;/h4&gt;
&lt;p&gt;Cost function의 선택은 심층 신경망 설계의 매우 중요한 측면 중 하나이다. 대부분의 경우 매개변수적 모형은 하나의 분포 $p(y|x;\theta)$를 정의한다. 그리고 최대가능도 원리를 적용해서 훈련을 진행한다. 이 경우 훈련 자료와 모형의 예측 사이의 교차 엔트로피를 비용함수로 사용하면 된다.
하지만 $y$에 관한 전체 확률 분포를 예측하는 대신 $x$를 조건으로 한 $y$의 어떤 통계량을 예측하는 좀 더 단순한 접근 방식이 바람직할 때도 있다. 이 경우, 특화된 손실함수를 이용하면 추정량을 예측하는 모형을 훈련할 수 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;최대가능도를 이용한 조건부 확률 학습&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;대부분의 신경망은 최대가능도를 사용해서 훈련한다. 이는 비용함수가 음의 로그가능도라는 뜻인데, 훈련 자료와 모형 분포 사이의 교차 엔트로피로도 동등하게 서술할 수 있다. 이 경우 비용함수는&lt;/p&gt;
&lt;center&gt;$J(\theta) = -E_{x,y \sim \hat{p}자료}\log p_{모형}(y|x)$&lt;/center&gt;
&lt;p&gt;이다. 
비용함수의 구체적인 형태는 모형마다 다르며, 특히 $\log p_{모형}$의 구체적인 형태에 의존한다. 최대가능도에서 비용함수를 유도하는 방식의 장점은, 모형마다 매번 비용함수를 설계하는 부담이 없다는 것이다. 모형 $p(y|x)$를 결정하기만 하면 비용함수 $\log p(y|x)$가 자동으로 결정된다. 
최대가능도 추정을 수행하는 데 쓰이는 교차 엔트로피 비용함수의 한 가지 독특한 성질은, 실제 응용에 흔히 쓰이는 모형들에서 이 함수에 최솟값이 없을 때가 많다는 것이다. 이산 출력 변수의 경우, 대부분의 모형은 정확히 확률값 0과 1을 표현하지 못하며 그 두 극단에 얼마든지 가까워질 수 있는 방식으로 매개변수화 된다. 실숫값 출력 변수의 경우에는, 만일 모형이 출력 분포의 밀도를 제어할 수 있다면, 정확한 훈련 집합 출력들에 극도로 높은 밀도를 배정할 여지가 생간다. 그러면 교차 엔트로피는 음의 무한대에 접근하게 된다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;조건부 통계량의 학습&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;모형이 전체 확률분포 $p(y|x;\theta)$를 학습하는것이 아니라 
$x$가 주어졌을 때의 $y$의 한 조건부 통계량만 학습하면 되는 경우도 있다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>ilvnax24er</name>
        
        
      </author>

      

      
        <category term="deeplearning" />
      

      
        <summary type="html">기울기 기반 학습 신경망은 비선형성이기 때문에 볼록함수를 손실함수로 사용하기가 적합하지 않ㅇ르 때가 많은것이 선형모형과 신경망의 가장 큰 차이점이다. 일반적으로 신경망에는 선형회귀 모형의 훈련에 쓰이는 연립방정식 해법이나 로지스틱 회귀 또는 SVM의 훈련에 쓰이는 전역 수렴을 보장하는 볼록함수 최적화 알고리즘 대신, 비용함수를 아주 낮은 값으로 이끄는 역할만 하는 반복적인 기울기 기반 최적화 절차를 사용한다. 볼록함수 최적화는 이론적으로 임의의 초기 매개변수들에서 시작해서 반드시 수렴한다. 비볼록 손실함수에 확률적 경사하강법을 적용할 때는 그러한 수렴이 보장되지 않으며, 결과가 초기 매개변수들에 민감하게 변한다. 순방향 신경망에서는 모든 가중치를 작은 난수들로 초기화하는 것이 중요하다.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">심층학습-Deep Feedforward Neural Network(1)</title>
      <link href="https://ilvnax24er.github.io/deeplearning-tutorial" rel="alternate" type="text/html" title="심층학습-Deep Feedforward Neural Network(1)" />
      <published>2021-06-28T00:00:00+09:00</published>
      <updated>2021-06-28T00:00:00+09:00</updated>
      <id>https://ilvnax24er.github.io/deeplearning-tutorial</id>
      <content type="html" xml:base="https://ilvnax24er.github.io/deeplearning-tutorial">&lt;h3 id=&quot;deep-feedforward-neural-network&quot;&gt;Deep Feedforward Neural Network&lt;/h3&gt;

&lt;p&gt;순방향 신경망의 목표는 어떤 함수 $f^{*}$를 근사하는 것이다.&lt;/p&gt;

&lt;p&gt;Classifier의 경우 function $y = f^{*}(x)$ 는 주어진 입력 $x$를 하나의 범주 또는 부류(class) $y$에 사상한다. 
순방향 신경망은 하나의 사상 $y = f(x;\theta)$를 정의하고,
실제 바탕 함수를 가장 잘 근사하는 매개변수 $\theta$의 값들을 학습한다.&lt;/p&gt;

&lt;p&gt;이 모형의 이름에 feedforward가 있는 이유는, 이 모형에서 정보가 앞쪽으로만 흘러가기 때문이다. 정보는 $x$를 입력으로 하여 평가되는 함수를 통과한 후 $f$를 정의하는데 쓰이는 중간 계산들을 거쳐서 최종적으로 출력 $y$에 도달한다. 이 모형에는 모형의 출력이 다시 모형 자신의 입력으로 투입되는 &lt;strong&gt;feedback&lt;/strong&gt;이 전혀 없다.&lt;/p&gt;

&lt;p&gt;순방향 신경망의 Network는 이 모형을 흔히 서로 다른 함수들이 그물처럼 엮인 형태로 표현한다는 점을 반영한 것이다. 자료 구조의 관점에서 이 모형은 함수들의 연결 관계를 표현하는 direct acyclic graph에 해당한다. 예를 들어 세 함수 $f^{1}, f^{2}, f^{3}$을 연쇄적으로 적용해서 $f(x) = f^{3}(f^{2}(f^{1}(x)))$라는 하나의 사슬 구조를 만들 수 있다.&lt;/p&gt;

&lt;p&gt;이때 $f^{1}$을 신경망의 first layer, $f^{2}$를 second layer, 등으로 부른다. 사슬의 전체 길이는 모형의 depth에 해당한다. 순방향 신경망의 마지막 층을 출력층(output layer)라 부르며 신경망을 훈련한다는 것은 $f(x)$ 를 $f^{*}(x)$에 적합시키는 것에 해당한다.&lt;/p&gt;

&lt;p&gt;$f^{*}$의 가장 좋은 근사값이 산출되도록 layer를 운용하는 방법은 학습 알고리즘이 결정해야한다. 그런 layer들의 바람직한 출력을 훈련 자료가 보여주지 않는다는 점에서, 그런 층들을 hidden layer라고 부른다.&lt;/p&gt;

&lt;p&gt;일반적으로 순방향 신경망의 각 vector를 입력받아서 vector를 출력하는 하나의 vector값 함수라 할 수 있다. 이러한 vector의 차원은 신경망의 width를 결정한다.&lt;/p&gt;

&lt;p&gt;선형 모형을 $x$의 비선형 함수들로 확장하는 한 방법은, 선형 모형을 $x$ 자체가 아니라 변환된 입력 $\phi(x)$에 적용하는 것이다. 여기서 $\phi$는 하나의 비선형 변환이다. 문제는 어떠한 $\phi$를 사용할 것인가다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;무한차원 $\phi$ 같은 아주 일반적인 $\phi$를 사용하는것이다. $\phi(x)$의 차원이 충분히 높으면, 모형은 항상 훈련 집합에 적합하기에 충분한 수용력을 가지게 된다. 그러나 시험집합으로의 일반화는 여전히 나쁠 때가 많다. 아주 일반적인 특징 사상들은 대체로 국소 불변성 원리에만 기초할 뿐, 고급 문제를 풀기에 충분한 정도의 사전 정보를 부호화하지 않을 때가 많다.&lt;/li&gt;
  &lt;li&gt;좀 더 특화된 $\phi$를 사람이 직접 고안하는 것이다. 심층학습이 등장하기 전에는 이것이 주도적인 접근 방식이었다. 이를 위해서는 개별 과제마다 사람이 수많은 노력을 통해 과제의 성격에 따라 음성인식이나 컴퓨터 시각 같은 서로 다른 응용 역역에 특화된 노력이 필요했다.&lt;/li&gt;
  &lt;li&gt;심층 학습이 사용하는 전략은 $\phi$를 배우는 것이다. 이 접근 방식에서 모형은 $y=f(x;\theta, \omega) = $\phi$(x;\theta)^{T}\omega$이다. 이는 심층 순방향 신경망의 한 예로, $\phi$는 하나의 은닉층을 정의한다. 이 접근방식은 훈련문제에 &lt;strong&gt;convexity&lt;/strong&gt;가 있으면 실패하는 방식이다. 하지만 이 접근방식은 모형의 표현을 $\phi(x;\theta)$로 매개변수화하고 최적화 알고리즘을 이용해서 좋은 표현에 해당하는 $\phi$를 구한다. 필요하다면 첫 접근 방식의 장점(고도의 일반성)을 도입해서 수용력을 높일 수도 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;순방향 신경망을 설계하려면 hidden layer의 값을 계산하는 데 사용할 activation function을 선택해야 한다. 또한 신경망의 architecture도 설계해야 한다. 이를 위한 결정 사항으로는 layer의 개수나 그 layer들의 상호 연결 방식, 각 layer의 계산 단위 개수등이 있다.&lt;/p&gt;

&lt;h3 id=&quot;xor-학습&quot;&gt;XOR 학습&lt;/h3&gt;

&lt;p&gt;XOR(exclusive or)함수는 두 이진수 $x_{1}$과 $x_{2}$에 대한 연산이다. 두 이진수 중 정확히 하나가 1과 같으면 XOR 함수는 1을 돌려주고, 그렇지 않으면 0을 돌려준다.
이 XOR함수가 바로 이 예제에서 학습할 목표함수 $y=f^{*}(x)$에 해당한다.&lt;/p&gt;

&lt;p&gt;이 예제의 모형은 함수 $y=f(x;\theta)$를 제공하며, 학습 알고리즘은 $f$가 $f^{*}$와 최대한 비슷해지도록 매개변수 $\theta$를 적합시킨다.&lt;/p&gt;

&lt;p&gt;우리의 목표는, 신경망이 네 자료점 $X = {[0, 0]^{T},[0,1]^{T},[1,0]^{T},[1,1]^{T}}$에 대해 제대로 작동하게 만드는 것이다. 이를 위해 네 점 모두로 신경망을 훈련한다.
이 문제를 하나의 회귀 문제로 간주해서 MSE를 손실함수로 사용하는 것이다.&lt;/p&gt;

&lt;p&gt;MSE = $J(\theta) = \frac{1}{4} \sum_{x \subseteq X} (f^{*}(x) - f(x;\theta))^{2}$&lt;/p&gt;

&lt;p&gt;다음으로 모형 $f(x;\theta)$의 구체적인 형식을 선택해야 한다. 모형이 선형이고 $\theta$가 $\omega$와 $b$로 구성하면 모형은 다음과 같이 정의된다.
$f(x;\omega ,b) = x^{T}\omega + b$&lt;/p&gt;

&lt;p&gt;이렇게 하면 $J(\theta)$를 표준방정식을 이용해서 $\omega$와 $b$에 대한 닫힌 형식으로 최소화할 수 있다. 표준방정식을 풀면 $\omega = 0$ 과 $b = \frac{1}{2}$이 나온다. 즉 선형 모형은 그냥 모든 점에서 0.5를 출력한다. 선형 모형을 원래의 입력에 직접 적용하면 XOR 함수를 배우지 못한다. $x_{1} = 0$일 떄는
$x_{2}$가 증가함에 따라 모형의 출력도 반드시 증가해야 하고,
$x_{1} = 1$일 때는
$x_{2}$가 증가함에 따라 모형의 출력이 반드시 감소해야 한다. 선형 모형은
$x_{2}$에 항상 고정된 계수
$\omega_{2}$를 적용할 뿐,
$x_{1}$의 값으로
$x_{2}$의 계수를 변경하지는 못하기 때문이 이 문제를 풀 수 없다.&lt;/p&gt;

&lt;p&gt;은닉 단위가 두 개인 은닉층 하나가 있는 간단한 순방향 신경망으로 이 문제를 해결할 수 있다. 이 순방향 신경망의 은닉층은 은닉 단위들을 담은 벡터 $h$로 구성된다. 신경망의 제 1층인 은닉층은 함수$f^{1}(x;W ,c)$를 계산하며, 그 출력이 제2층인 출력층에 전달된다. 출력층은 이전처럼 그냥 하나의 선형회귀 모형인데, $x$가 아니라 $h$에 대해 작용한다는 것이 이전모형과 다른 점이다. 이러한 층들로 구성된 신경망은 두 함수 $h = f^{1}(x;W,c)$와 $y=f^{2}(h;\omega ,b)$를 사슬로 연결한 형태이다. 즉, 최종 모형은 $f(x;W,c,\omega ,b) = f^{2}(f^{1}(x))$이다.&lt;/p&gt;

&lt;p&gt;그런데 어떤 함수를 $f^{1}$로 두어야 할까? 지금까지는 선형 모형들이 잘 작동했으므로 $f^{1}$도 선형으로 두면 될 것 같다. 그러나 $f^{1}$이 선형이면 순방향 신경망 전체가 주어진 입력의 선형 함수가 된다. $f^{1}(x) = W^{T}x$이고 $f^{2}(h) = h^{T}\omega$이면 $f(x) = \omega^{T}W^{T}x$이다. $\omega^{‘} = W\omega$로 두면 이 함수를 $f(x) = x^{T}\omega^{‘}$으로 표현할 수 있는데, 이는 선형이다.&lt;/p&gt;

&lt;p&gt;완전한 신경망을 표현하면&lt;/p&gt;

&lt;p&gt;$f(x;W.c.\omega,b) = \omega^{T}max{0,W^{T}x+c}+b$ 이다.&lt;/p&gt;

&lt;p&gt;이제 XOR 문제를 풀어보면,&lt;/p&gt;

&lt;center&gt;$W = \begin{bmatrix}
1&amp;amp;1 \\\
1&amp;amp;1
\end{bmatrix}$&lt;/center&gt;

&lt;center&gt;$c = \begin{bmatrix}
0 \\\
-1
\end{bmatrix}$&lt;/center&gt;

&lt;center&gt;$\omega = \begin{bmatrix}
1 \\\
-2
\end{bmatrix}$&lt;/center&gt;

&lt;center&gt;$b=0$&lt;/center&gt;

&lt;p&gt;다음으로, 모형이 입력 전체를 처리하는 과정을 보면, 입력 $X$는 이진 입력 공간의 네 점 모두를 담은 설계 행렬로, 하나의 행은 하나의 견본에 해당한다.&lt;/p&gt;

&lt;center&gt;$X = \begin{bmatrix}
0&amp;amp;0\\\
0&amp;amp;1\\\
1&amp;amp;0\\\
1&amp;amp;1
\end{bmatrix}$&lt;/center&gt;

&lt;center&gt;$XW = \begin{bmatrix}
0&amp;amp;0\\\
1&amp;amp;1\\\
1&amp;amp;1\\\
2&amp;amp;2
\end{bmatrix}$&lt;/center&gt;

&lt;p&gt;여기에 vector $c$를 더하면,&lt;/p&gt;

&lt;center&gt;$XW = \begin{bmatrix}
0&amp;amp;-1\\
1&amp;amp;0\\
1&amp;amp;0\\
2&amp;amp;1
\end{bmatrix}$&lt;/center&gt;

&lt;p&gt;이 공간에서 모든 견본은 기울기가 1인 직선에 놓여있다. 그 직선을 따라갈 때 함수의 출력은 0에서 시작해서 1로 올라갔다가 다시 0으로 떨어진다. 선형 모형으로는 그런 함수를 구현할 수 없다. 각 견본에 대한 $h$의 값을 산출하기 위해, 지금까지의 결과에 다음과 같은 정류 선형 변환을 적용한다.&lt;/p&gt;

&lt;center&gt;$ \begin{bmatrix}
0&amp;amp;0\\\
1&amp;amp;0\\\
1&amp;amp;0\\\
2&amp;amp;1
\end{bmatrix}$&lt;/center&gt;

&lt;p&gt;이 변환을 적용하면 견본들 사이의 관계가 변한다. 이제는 견본들이 하나의 직선에 놓인 것이 아니라 선형으로 문제를 풀 수 있는 형태의 공간에 놓이게 된다.
마지막으로, 다음과 같은 가중치 벡트 $\omega$를 곱한다.&lt;/p&gt;

&lt;center&gt;$\begin{bmatrix}
0\\\
1\\\
1\\\
0
\end{bmatrix}$&lt;/center&gt;

&lt;p&gt;이렇게 하면 신경망은 입력 행렬의 모든 견본에 대해 정확한 답을 낸다.&lt;/p&gt;

&lt;p&gt;그러나 실제 응용에서는 모형 매개변수 개수와 훈련 견본 개수가 수십억 규모라서 지금처럼 해를 바로 도출하지 못할 수 있다.
그런 상황에서는 기울기 기반 최적화 알고리즘을 이용해서 오차가 아주 작은 매개변수들을 찾는 것이 한 방법이다.
XOR문제의 해는 손실함수의 한 전역 최소점에 해당하므로, 경사 하강법을 이용해서 그 점으로 수렴하는 것이 가능하다.
XOR문제의 다른 동등한 해들도 경사 하강법으로 구할 수 있다. 경사 하강의 수렴점은 매개변수들의 초깃값에 의존한다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>ilvnax24er</name>
        
        
      </author>

      

      
        <category term="deeplearning" />
      

      
        <summary type="html">Deep Feedforward Neural Network</summary>
      

      
      
    </entry>
  
</feed>
